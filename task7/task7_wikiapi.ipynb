{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pkCL7JmetpqW",
        "outputId": "0d18e21c-3268-49f9-b5a5-b1b3a549947c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for wikipedia-api (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ],
      "source": [
        "!pip install -q wikipedia-api"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import wikipediaapi\n",
        "import json"
      ],
      "metadata": {
        "id": "mXh32s_FtuFJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "USER_AGENT = \"task7-semantic-search/1.0 (ozbekk.cagatay@gmail.com)\"\n",
        "# Türkçe ve İngilizce Wikipedia API objeleri\n",
        "\n",
        "wiki_tr = wikipediaapi.Wikipedia(user_agent=USER_AGENT, language='tr')\n",
        "wiki_en = wikipediaapi.Wikipedia(user_agent=USER_AGENT, language='en')\n",
        "\n",
        "# Çekilecek makale başlık listeleri (örnek)\n",
        "titles_tr = [\n",
        "    \"Yapay zeka\"\n",
        "]\n",
        "titles_en = [\n",
        "    \"Artificial intelligence\"]\n"
      ],
      "metadata": {
        "id": "raBuH3oatzyO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_page_text(wiki, title):\n",
        "    page = wiki.page(title)\n",
        "    return page.text if page.exists() else None\n",
        "\n",
        "# Dokümanları topla\n",
        "docs = []\n",
        "for t in titles_tr:\n",
        "    text = get_page_text(wiki_tr, t)\n",
        "    if text:\n",
        "        docs.append({\"title\": t, \"lang\": \"tr\", \"text\": text})\n",
        "\n",
        "for t in titles_en:\n",
        "    text = get_page_text(wiki_en, t)\n",
        "    if text:\n",
        "        docs.append({\"title\": t, \"lang\": \"en\", \"text\": text})\n",
        "\n",
        "print(f\"Toplam {len(docs)} doküman toplandı.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aG1vKSyevEn9",
        "outputId": "f1713c34-b20e-4e76-b3bd-131c29c438ed"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Toplam 2 doküman toplandı.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Kaydet\n",
        "output_path = \"/content/raw_documents.jsonl\"\n",
        "with open(output_path, \"w\", encoding=\"utf-8\") as f:\n",
        "    for doc in docs:\n",
        "        f.write(json.dumps(doc, ensure_ascii=False) + \"\\n\")\n",
        "\n",
        "print(f\"Veriler kaydedildi: {output_path}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zii9RkRovGML",
        "outputId": "90b3b1bc-8b1d-412c-c047-67878a2c0ad7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Veriler kaydedildi: /content/raw_documents.jsonl\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# === Parametreler ===\n",
        "RAW_PATH   = \"/content/drive/MyDrive/raw_documents.jsonl\"  # bir önceki adımın çıktısı\n",
        "CHUNK_PATH = \"/content/drive/MyDrive/chunks.jsonl\"\n",
        "MAX_TOKENS = 350    # ~200–500 arası önerilir\n",
        "OVERLAP    = 50     # bağlam korumak için küçük overlap\n"
      ],
      "metadata": {
        "id": "3VayqiPcvLA8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# === Chunklama Yardımcıları ===\n",
        "import re, json, os, unicodedata\n",
        "\n",
        "def slugify(title: str) -> str:\n",
        "    s = unicodedata.normalize(\"NFKD\", title).encode(\"ascii\", \"ignore\").decode(\"ascii\")\n",
        "    s = re.sub(r\"[^a-zA-Z0-9]+\", \"-\", s).strip(\"-\").lower()\n",
        "    return s or \"doc\"\n",
        "\n",
        "def sent_split(text: str):\n",
        "    text = re.sub(r\"\\s+\", \" \", text.strip())\n",
        "    # basit ve güçlü bir cümle bölücü: . ? ! ; : sonları + satır sonları\n",
        "    sents = re.split(r\"(?<=[\\.\\?\\!\\;\\:])\\s+|\\n+\", text)\n",
        "    return [s for s in sents if s]\n",
        "\n",
        "def tokenize(s: str):\n",
        "    return s.split()\n",
        "\n",
        "def chunkify_by_tokens(sents, max_tokens=350, overlap=50):\n",
        "    chunks, cur, cur_len = [], [], 0\n",
        "    for s in sents:\n",
        "        toks = tokenize(s)\n",
        "        L = len(toks)\n",
        "        if cur_len + L <= max_tokens:\n",
        "            cur.append(s); cur_len += L\n",
        "        else:\n",
        "            if cur:\n",
        "                chunks.append(\" \".join(cur))\n",
        "                # overlap uygula\n",
        "                if overlap > 0:\n",
        "                    last_tokens = tokenize(\" \".join(cur))[-overlap:]\n",
        "                    cur = [\" \".join(last_tokens)]\n",
        "                    cur_len = len(last_tokens)\n",
        "                else:\n",
        "                    cur, cur_len = [], 0\n",
        "            # bu cümle tek başına büyükse parçala\n",
        "            if L > max_tokens:\n",
        "                for i in range(0, L, max_tokens):\n",
        "                    chunks.append(\" \".join(toks[i:i+max_tokens]))\n",
        "                cur, cur_len = [], 0\n",
        "            else:\n",
        "                cur.append(s); cur_len += L\n",
        "    if cur:\n",
        "        chunks.append(\" \".join(cur))\n",
        "    return chunks"
      ],
      "metadata": {
        "id": "g2t2FstIvvi0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# === Oku, chunkla, yaz ===\n",
        "docs = []\n",
        "with open(RAW_PATH, \"r\", encoding=\"utf-8\") as f:\n",
        "    for line in f:\n",
        "        docs.append(json.loads(line))\n",
        "\n",
        "os.makedirs(os.path.dirname(CHUNK_PATH), exist_ok=True)\n",
        "n_docs, n_chunks = 0, 0\n",
        "\n",
        "with open(CHUNK_PATH, \"w\", encoding=\"utf-8\") as w:\n",
        "    for d in docs:\n",
        "        title = d.get(\"title\", f\"doc{n_docs}\")\n",
        "        lang  = d.get(\"lang\", \"en\")\n",
        "        text  = d.get(\"text\", \"\")\n",
        "        if not text.strip():\n",
        "            continue\n",
        "        n_docs += 1\n",
        "        doc_id = f\"{slugify(title)}\"\n",
        "        sents  = sent_split(text)\n",
        "        parts  = chunkify_by_tokens(sents, MAX_TOKENS, OVERLAP)\n",
        "        for i, ch in enumerate(parts):\n",
        "            rec = {\n",
        "                \"doc_id\": doc_id,\n",
        "                \"chunk_id\": i,\n",
        "                \"text\": ch,\n",
        "                \"language\": lang,\n",
        "                \"source\": title\n",
        "            }\n",
        "            w.write(json.dumps(rec, ensure_ascii=False) + \"\\n\")\n",
        "            n_chunks += 1\n",
        "\n",
        "print(f\"Tamam. {n_docs} dokümandan toplam {n_chunks} chunk yazıldı -> {CHUNK_PATH}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7J-_zEfGvz22",
        "outputId": "5b4c7ab4-7f41-4ce9-9233-26d000531bdf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tamam. 13 dokümandan toplam 395 chunk yazıldı -> /content/drive/MyDrive/chunks.jsonl\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json, statistics\n",
        "\n",
        "CHUNK_PATH = \"/content/drive/MyDrive/chunks.jsonl\"\n",
        "\n",
        "samples, lengths, by_doc = [], [], {}\n",
        "with open(CHUNK_PATH, \"r\", encoding=\"utf-8\") as f:\n",
        "    for k, line in enumerate(f):\n",
        "        obj = json.loads(line)\n",
        "        txt = obj[\"text\"]\n",
        "        tok = len(txt.split())\n",
        "        lengths.append(tok)\n",
        "        key = obj[\"doc_id\"]\n",
        "        by_doc[key] = by_doc.get(key, 0) + 1\n",
        "        if len(samples) < 3:  # 3 örnek göster\n",
        "            samples.append(obj)\n",
        "\n",
        "print(f\"Toplam chunk: {len(lengths)}\")\n",
        "if lengths:\n",
        "    print(f\"Ortalama token/chunk: {statistics.mean(lengths):.1f}\")\n",
        "    print(f\"Medyan token/chunk:   {statistics.median(lengths):.1f}\")\n",
        "    print(f\"Min/Max token:        {min(lengths)} / {max(lengths)}\")\n",
        "\n",
        "print(\"\\nÖrnek 3 chunk:\")\n",
        "for s in samples:\n",
        "    print(f\"- {s['doc_id']}#{s['chunk_id']} [{s['language']}]: {s['text'][:200]}...\")\n",
        "\n",
        "print(\"\\nDoküman başına chunk sayısı (ilk 10):\")\n",
        "for i, (k, v) in enumerate(sorted(by_doc.items(), key=lambda x: -x[1])[:10], start=1):\n",
        "    print(f\"{i:>2}. {k}: {v}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6Tvb8pX4v4cT",
        "outputId": "fafb6115-d8db-4558-b7fc-8faca90854bd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Toplam chunk: 395\n",
            "Ortalama token/chunk: 334.3\n",
            "Medyan token/chunk:   340.0\n",
            "Min/Max token:        140 / 350\n",
            "\n",
            "Örnek 3 chunk:\n",
            "- yapay-zeka#0 [tr]: Yapay zekâ (İngilizce: Artificial intelligence, AI); hesaplama sistemlerinin öğrenme, akıl, problem çözme, algılama ve karar verme gibi insan zekâsıyla tipik olarak ilişkilendirilen görevleri yerine g...\n",
            "- yapay-zeka#1 [tr]: eserlerinde dışa vurmuştur. Karel Čapek, R.U.R. adlı tiyatro oyununda yapay zekâya sahip robotlar ile insanlığın ortak toplumsal sorunlarını ele alarak 1920 yılında yapay zekânın insan aklından bağıms...\n",
            "- yapay-zeka#2 [tr]: birçok alanda kullanılmaktadır. Tarihçe Yapay zekâ tarihi, antik çağlarda, usta zanaatkarlar tarafından zeka veya bilinç kazandırılan yapay varlıklara ilişkin mitler, hikâyeler ve söylentilerle başlad...\n",
            "\n",
            "Doküman başına chunk sayısı (ilk 10):\n",
            " 1. london: 53\n",
            " 2. ottoman-empire: 53\n",
            " 3. economy-of-the-united-states: 51\n",
            " 4. artificial-intelligence: 47\n",
            " 5. osmanl-imparatorlugu: 46\n",
            " 6. real-madrid-cf: 45\n",
            " 7. besiktas-jimnastik-kulubu: 26\n",
            " 8. python-programming-language: 24\n",
            " 9. turkiye-ekonomisi: 18\n",
            "10. ankara: 13\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "k6umNvr0v74X"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}